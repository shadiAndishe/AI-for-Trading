{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN3_3(2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVukvOR4jMA-"
      },
      "source": [
        "# train a generative adversarial network on a one-dimensional function\r\n",
        "from numpy import hstack\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from numpy import zeros\r\n",
        "from numpy import ones\r\n",
        "from numpy.random import rand\r\n",
        "from numpy.random import randn\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from keras.layers import Input\r\n",
        "from keras.models import Sequential,Model\r\n",
        "from keras.layers import Dense, Conv1D, Dropout, BatchNormalization, LeakyReLU , Flatten ,LSTM, Embedding\r\n",
        "from matplotlib import pyplot\r\n",
        "import pandas as pd\r\n",
        "from datetime import datetime, timedelta\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn import preprocessing\r\n",
        "from sklearn.preprocessing import PowerTransformer\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import math\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUtdDPF0jP64"
      },
      "source": [
        "  # define the standalone discriminator model\r\n",
        "\r\n",
        "def define_discriminator(n_inputs=3):\r\n",
        "  \r\n",
        "    \r\n",
        "\r\n",
        "    discriminator=Sequential()\r\n",
        "    discriminator.add(Dense(units=512,input_dim=n_inputs))\r\n",
        "    discriminator.add(LeakyReLU(0.2))\r\n",
        "    discriminator.add(Dropout(0.3))\r\n",
        "       \r\n",
        "    \r\n",
        "    discriminator.add(Dense(units=256))\r\n",
        "    discriminator.add(LeakyReLU(0.2))\r\n",
        "    discriminator.add(Dropout(0.3))\r\n",
        "       \r\n",
        "    discriminator.add(Dense(units=128))\r\n",
        "    discriminator.add(LeakyReLU(0.2))\r\n",
        "    \r\n",
        "    discriminator.add(Dense(units=1, activation='sigmoid'))\r\n",
        "    \r\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "    return discriminator\r\n",
        "\r\n",
        "    discriminator.summary()\r\n",
        "\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    #model.add(Dense(128,return_sequences='true', input_shape=(3, 128)))\r\n",
        "      #model.add(LSTM(128,return_sequences= True, input_shape=(3, 128)))\r\n",
        "      #model.add(Dense(1024, activation='softmax'))\r\n",
        "\r\n",
        "    \r\n",
        "      #model.add(LSTM(3,return_sequences=True, input_shape=(128,3)))\r\n",
        "      #model.add(Dense(1, activation='linear'))\r\n",
        "    \r\n",
        "      # compile model\r\n",
        "      #model.compile(loss='mse', optimizer='adam')'''\r\n",
        "    \r\n",
        "  "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-12cX7djhmo"
      },
      "source": [
        "# define the standalone generator model\r\n",
        "def define_generator(latent_dim, n_outputs=3):\r\n",
        "  \r\n",
        "    generator=Sequential()\r\n",
        "    generator.add(Dense(units=128,input_dim=latent_dim))\r\n",
        "    generator.add(LeakyReLU(0.2))\r\n",
        "    \r\n",
        "    generator.add(Dense(units=256))\r\n",
        "    generator.add(LeakyReLU(0.2))\r\n",
        "    \r\n",
        "    generator.add(Dense(units=512))\r\n",
        "    generator.add(LeakyReLU(0.2))\r\n",
        "\r\n",
        "  \r\n",
        "    \r\n",
        "    generator.add(Dense(units=3, activation='tanh'))\r\n",
        "    \r\n",
        "    generator.compile(loss='binary_crossentropy', optimizer='adam')\r\n",
        "    return generator\r\n",
        "\r\n",
        "    generator.summary()\r\n",
        "   \r\n",
        "\r\n",
        "\r\n",
        " "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQLWLFqBjkY6"
      },
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\r\n",
        "def define_gan(generator, discriminator):\r\n",
        "\t# make weights in the discriminator not trainable\r\n",
        "\t#discriminator.trainable = False\r\n",
        "\t# connect them\r\n",
        "\t#model = Sequential()\r\n",
        "\t# add generator\r\n",
        "\t#model.add(generator)\r\n",
        "\t# add the discriminator\r\n",
        "\t#model.add(discriminator)\r\n",
        "\t# compile model\r\n",
        "\t#model.compile(loss='binary_crossentropy', optimizer='adam')\r\n",
        "\t#return model\r\n",
        "\r\n",
        "  discriminator.trainable = False\r\n",
        "  gan_input = Input(shape=(4,))\r\n",
        "  x = generator(gan_input)\r\n",
        "  gan_output= discriminator(x)\r\n",
        "  gan= Model(inputs=gan_input, outputs=gan_output)\r\n",
        "  gan.compile(loss='binary_crossentropy', optimizer='adam')\r\n",
        "  return gan\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMRgnCp55MUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f2168e-3767-4a08-abc3-6b7a548875ea"
      },
      "source": [
        "# Google drive\r\n",
        "GDRIVE_DIR = \"/content/gdrive\" # Your own mount point on Google Drive\r\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/My Drive\" # Your own home directory\r\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/BDCDataSet/\" # Your own data directory\r\n",
        "from google.colab import drive\r\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)\r\n",
        "###################################################\r\n",
        "#Read Data\r\n",
        "COLUMNS_NAMES = {\"orderbook\": [\"sell\", \"vsell\", \"buy\", \"vbuy\"],\r\n",
        "                 \"message\": [\"time\", \"event_type\", \"order_id\", \"size\", \"price\", \"direction\"]}\r\n",
        "\r\n",
        "messages = pd.read_csv(\"/content/gdrive/My Drive/MMM/AAPL_2012-06-21_34200000_57600000_message_1.csv\", names=COLUMNS_NAMES[\"message\"])\r\n",
        "orderbook = pd.read_csv(\"/content/gdrive/My Drive/MMM/AAPL_2012-06-21_34200000_57600000_orderbook_1.csv\", names=COLUMNS_NAMES[\"orderbook\"])\r\n",
        "\r\n",
        "#print(\"History messages\")\r\n",
        "#print(messages)\r\n",
        "#print(\"Orderbook\")\r\n",
        "#print(orderbook)\r\n",
        "###################################################\r\n",
        "#Merge Data\r\n",
        "df_combined = messages.copy()\r\n",
        "df_combined[COLUMNS_NAMES[\"orderbook\"]] = orderbook\r\n",
        "#print(df_combined)\r\n",
        "###################################################\r\n",
        "df_combined = df_combined[(df_combined[\"event_type\"].isin([4,5]))]\r\n",
        "#print(df_combined)\r\n",
        "###################################################\r\n",
        "#Drop Not Useful Coloumns\r\n",
        "Final_df=df_combined.copy()\r\n",
        "Final_df.drop(['order_id', 'event_type', 'sell', 'vsell', 'buy', 'vbuy','time',], axis=1, inplace=True)\r\n",
        "print(Final_df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "        size    price  direction\n",
            "7         40  5857400         -1\n",
            "8         25  5857500         -1\n",
            "9          1  5857300          1\n",
            "10        10  5857300          1\n",
            "11        25  5857500         -1\n",
            "...      ...      ...        ...\n",
            "118488    89  5775500          1\n",
            "118492   103  5776000          1\n",
            "118493    11  5776000          1\n",
            "118495    48  5776100         -1\n",
            "118496    52  5776350         -1\n",
            "\n",
            "[34990 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2u3NXzFMVUY",
        "outputId": "95a5d8d9-f81f-46de-becf-77bbfdaad9c4"
      },
      "source": [
        "#Read Data\r\n",
        "COLUMNS_NAMES = {\"orderbook\": [\"sell\", \"vsell\", \"buy\", \"vbuy\"],\r\n",
        "                 \"message\": [\"time\", \"event_type\", \"order_id\", \"size\", \"price\", \"direction\"]}\r\n",
        "\r\n",
        "messages1 = pd.read_csv(\"/content/gdrive/My Drive/MMM/MSFT_2012-06-21_34200000_57600000_message_1.csv\", names=COLUMNS_NAMES[\"message\"])\r\n",
        "orderbook1 = pd.read_csv(\"/content/gdrive/My Drive/MMM/MSFT_2012-06-21_34200000_57600000_orderbook_1.csv\", names=COLUMNS_NAMES[\"orderbook\"])\r\n",
        "\r\n",
        "#print(\"History messages\")\r\n",
        "#print(messages1)\r\n",
        "#print(\"Orderbook\")\r\n",
        "#print(orderbook1)\r\n",
        "###################################################\r\n",
        "#Merge Data\r\n",
        "df_combined1 = messages1.copy()\r\n",
        "df_combined1[COLUMNS_NAMES[\"orderbook\"]] = orderbook1\r\n",
        "#print(df_combined1)\r\n",
        "###################################################\r\n",
        "df_combined1 = df_combined1[(df_combined1[\"event_type\"].isin([4,5]))]\r\n",
        "#print(df_combined1)\r\n",
        "###################################################\r\n",
        "#Drop Not Useful Coloumns\r\n",
        "Final_df1=df_combined1.copy()\r\n",
        "Final_df1.drop(['order_id', 'event_type', 'sell', 'vsell', 'buy', 'vbuy','time',], axis=1, inplace=True)\r\n",
        "print(Final_df1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        size   price  direction\n",
            "0        300  309600         -1\n",
            "1         27  309700         -1\n",
            "8        100  309700         -1\n",
            "9        100  309700         -1\n",
            "10       100  309700         -1\n",
            "...      ...     ...        ...\n",
            "411385   400  301400         -1\n",
            "411389   400  301350         -1\n",
            "411391     1  301300          1\n",
            "411407   100  301350         -1\n",
            "411408   300  301350         -1\n",
            "\n",
            "[33414 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoMeLxqOhgz-",
        "outputId": "8e273790-0008-434b-c511-11b254fa10b0"
      },
      "source": [
        "# Google drive\r\n",
        "GDRIVE_DIR = \"/content/gdrive\" # Your own mount point on Google Drive\r\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/My Drive\" # Your own home directory\r\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/BDCDataSet/\" # Your own data directory\r\n",
        "from google.colab import drive\r\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)\r\n",
        "###################################################\r\n",
        "#Read Data\r\n",
        "COLUMNS_NAMES = {\"orderbook\": [\"sell\", \"vsell\", \"buy\", \"vbuy\"],\r\n",
        "                 \"message\": [\"time\", \"event_type\", \"order_id\", \"size\", \"price\", \"direction\"]}\r\n",
        "\r\n",
        "messages2 = pd.read_csv(\"/content/gdrive/My Drive/MMM/AMZN_2012-06-21_34200000_57600000_message_1.csv\", names=COLUMNS_NAMES[\"message\"])\r\n",
        "orderbook2 = pd.read_csv(\"/content/gdrive/My Drive/MMM/AMZN_2012-06-21_34200000_57600000_orderbook_1.csv\", names=COLUMNS_NAMES[\"orderbook\"])\r\n",
        "\r\n",
        "#print(\"History messages\")\r\n",
        "#print(messages2)\r\n",
        "#print(\"Orderbook\")\r\n",
        "#print(orderbook2)\r\n",
        "###################################################\r\n",
        "#Merge Data\r\n",
        "df_combined2 = messages2.copy()\r\n",
        "df_combined2[COLUMNS_NAMES[\"orderbook\"]] = orderbook2\r\n",
        "#print(df_combined2)\r\n",
        "###################################################\r\n",
        "df_combined2 = df_combined2[(df_combined2[\"event_type\"].isin([4,5]))]\r\n",
        "#print(df_combined2)\r\n",
        "###################################################\r\n",
        "#Drop Not Useful Coloumns\r\n",
        "Final_df2=df_combined2.copy()\r\n",
        "Final_df2.drop(['order_id', 'event_type', 'sell', 'vsell', 'buy', 'vbuy','time',], axis=1, inplace=True)\r\n",
        "print(Final_df2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "       size    price  direction\n",
            "0         1  2238200         -1\n",
            "2        21  2238100          1\n",
            "3        26  2237500          1\n",
            "4       100  2238400         -1\n",
            "5       100  2238400         -1\n",
            "...     ...      ...        ...\n",
            "57504   100  2205200          1\n",
            "57506     9  2205300         -1\n",
            "57508    91  2205100          1\n",
            "57510     9  2205100          1\n",
            "57511    51  2205100          1\n",
            "\n",
            "[11419 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAFWeG97hPMw",
        "outputId": "6e0ea060-a469-4940-ae50-6537866ad017"
      },
      "source": [
        "# Google drive\r\n",
        "GDRIVE_DIR = \"/content/gdrive\" # Your own mount point on Google Drive\r\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/My Drive\" # Your own home directory\r\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/BDCDataSet/\" # Your own data directory\r\n",
        "from google.colab import drive\r\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)\r\n",
        "###################################################\r\n",
        "#Read Data\r\n",
        "COLUMNS_NAMES = {\"orderbook\": [\"sell\", \"vsell\", \"buy\", \"vbuy\"],\r\n",
        "                 \"message\": [\"time\", \"event_type\", \"order_id\", \"size\", \"price\", \"direction\"]}\r\n",
        "\r\n",
        "messages3 = pd.read_csv(\"/content/gdrive/My Drive/MMM/INTC_2012-06-21_34200000_57600000_message_1.csv\", names=COLUMNS_NAMES[\"message\"])\r\n",
        "orderbook3 = pd.read_csv(\"/content/gdrive/My Drive/MMM/INTC_2012-06-21_34200000_57600000_orderbook_1.csv\", names=COLUMNS_NAMES[\"orderbook\"])\r\n",
        "\r\n",
        "#print(\"History messages\")\r\n",
        "#print(messages3)\r\n",
        "#print(\"Orderbook\")\r\n",
        "#print(orderbook3)\r\n",
        "###################################################\r\n",
        "#Merge Data\r\n",
        "df_combined3 = messages3.copy()\r\n",
        "df_combined3[COLUMNS_NAMES[\"orderbook\"]] = orderbook3\r\n",
        "#print(df_combined3)\r\n",
        "###################################################\r\n",
        "df_combined3 = df_combined3[(df_combined3[\"event_type\"].isin([4,5]))]\r\n",
        "#print(df_combined3)\r\n",
        "###################################################\r\n",
        "#Drop Not Useful Coloumns\r\n",
        "Final_df3=df_combined3.copy()\r\n",
        "Final_df3.drop(['order_id', 'event_type', 'sell', 'vsell', 'buy', 'vbuy','time',], axis=1, inplace=True)\r\n",
        "print(Final_df3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "        size   price  direction\n",
            "0         34  275200         -1\n",
            "3         66  275200         -1\n",
            "4        100  275200         -1\n",
            "5        125  275200         -1\n",
            "43       100  275200         -1\n",
            "...      ...     ...        ...\n",
            "404920   100  267100          1\n",
            "404934  3700  267100          1\n",
            "404952   500  267100          1\n",
            "404954  3800  267100          1\n",
            "404985  3800  267100          1\n",
            "\n",
            "[32483 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmXHV_-7iB_G",
        "outputId": "0bcbbbaa-fd12-4119-883f-4edffd75a250"
      },
      "source": [
        "# Google drive\r\n",
        "GDRIVE_DIR = \"/content/gdrive\" # Your own mount point on Google Drive\r\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/My Drive\" # Your own home directory\r\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/BDCDataSet/\" # Your own data directory\r\n",
        "from google.colab import drive\r\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)\r\n",
        "###################################################\r\n",
        "#Read Data\r\n",
        "COLUMNS_NAMES = {\"orderbook\": [\"sell\", \"vsell\", \"buy\", \"vbuy\"],\r\n",
        "                 \"message\": [\"time\", \"event_type\", \"order_id\", \"size\", \"price\", \"direction\"]}\r\n",
        "\r\n",
        "messages4 = pd.read_csv(\"/content/gdrive/My Drive/MMM/GOOG_2012-06-21_34200000_57600000_message_1.csv\", names=COLUMNS_NAMES[\"message\"])\r\n",
        "orderbook4 = pd.read_csv(\"/content/gdrive/My Drive/MMM/GOOG_2012-06-21_34200000_57600000_orderbook_1.csv\", names=COLUMNS_NAMES[\"orderbook\"])\r\n",
        "\r\n",
        "#print(\"History messages\")\r\n",
        "#print(messages4)\r\n",
        "#print(\"Orderbook\")\r\n",
        "#print(orderbook4)\r\n",
        "###################################################\r\n",
        "#Merge Data\r\n",
        "df_combined4 = messages4.copy()\r\n",
        "df_combined4[COLUMNS_NAMES[\"orderbook\"]] = orderbook4\r\n",
        "#print(df_combined4)\r\n",
        "###################################################\r\n",
        "df_combined4 = df_combined4[(df_combined4[\"event_type\"].isin([4,5]))]\r\n",
        "#print(df_combined4)\r\n",
        "###################################################\r\n",
        "#Drop Not Useful Coloumns\r\n",
        "Final_df4=df_combined4.copy()\r\n",
        "Final_df4.drop(['order_id', 'event_type', 'sell', 'vsell', 'buy', 'vbuy','time',], axis=1, inplace=True)\r\n",
        "print(Final_df4)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "       size    price  direction\n",
            "0         4  5794000          1\n",
            "1       300  5794000          1\n",
            "2         1  5795100          1\n",
            "3         1  5795000          1\n",
            "4         1  5794900          1\n",
            "...     ...      ...        ...\n",
            "49471    25  5652100          1\n",
            "49473    75  5652100          1\n",
            "49476   188  5653000          1\n",
            "49477   200  5652400          1\n",
            "49479   100  5653500          1\n",
            "\n",
            "[11678 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G15Bw-sFO9k3",
        "outputId": "dc8c35ed-aa09-47f5-a07b-5d6ad73e26cc"
      },
      "source": [
        "frames = [Final_df, Final_df1, Final_df2, Final_df3, Final_df4]\r\n",
        "result = pd.concat(frames)\r\n",
        "print(result)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       size    price  direction\n",
            "7        40  5857400         -1\n",
            "8        25  5857500         -1\n",
            "9         1  5857300          1\n",
            "10       10  5857300          1\n",
            "11       25  5857500         -1\n",
            "...     ...      ...        ...\n",
            "49471    25  5652100          1\n",
            "49473    75  5652100          1\n",
            "49476   188  5653000          1\n",
            "49477   200  5652400          1\n",
            "49479   100  5653500          1\n",
            "\n",
            "[123984 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5pthQWfxNoZ",
        "outputId": "947cb3d5-9875-4283-b2a1-2ee6ee0b5844"
      },
      "source": [
        "X1 = result['size']\r\n",
        "X2 = result['price']\r\n",
        "X3 = result['direction']\r\n",
        "X1=X1.to_numpy()\r\n",
        "X2=X2.to_numpy()\r\n",
        "X3=X3.to_numpy()\r\n",
        "X1 = X1.reshape(123984 , 1)\r\n",
        "X2 = X2.reshape(123984 , 1)\r\n",
        "X3 = X3.reshape(123984 , 1)\r\n",
        "X2=X2/10000\r\n",
        "maxX1=5000\r\n",
        "minX1=np.amin(X1)\r\n",
        "maxX2=np.amax(X2)\r\n",
        "minX2=np.amin(X2)\r\n",
        "maxX3=np.amax(X3)\r\n",
        "minX3=np.amin(X3)\r\n",
        "scaler = MinMaxScaler()\r\n",
        "scaler.fit(X1)\r\n",
        "X1N=scaler.transform(X1)\r\n",
        "scaler.fit(X2)\r\n",
        "X2N=scaler.transform(X2)\r\n",
        "scaler.fit(X3)\r\n",
        "#X3N=scaler.transform(X3)\r\n",
        "X3N=X3\r\n",
        "print(maxX1)\r\n",
        "print(minX1)\r\n",
        "print(maxX2)\r\n",
        "print(minX2)\r\n",
        "print(maxX3)\r\n",
        "print(minX3)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "1\n",
            "588.21\n",
            "26.61\n",
            "1\n",
            "-1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bSnT5kk6Cg7"
      },
      "source": [
        "# generate n real samples with class labels                                    OK\r\n",
        "def generate_real_samples(n):\r\n",
        "  #X1 = Final_df['size'][100*i:(100*i)+100]\r\n",
        "  #X2 = Final_df['price'][100*i:(100*i)+100]\r\n",
        "  #X3 = Final_df['direction'][100*i:(100*i)+100]\r\n",
        "  #X1 = Final_df['size']\r\n",
        "  #X2 = Final_df['price']\r\n",
        "  #X3 = Final_df['direction']\r\n",
        "  #X1=X1.to_numpy()\r\n",
        "  #X2=X2.to_numpy()\r\n",
        "  #X3=X3.to_numpy()\r\n",
        "  #X1 = X1.reshape(n, 1)\r\n",
        "  #X2 = X2.reshape(n, 1)\r\n",
        "  #X3 = X3.reshape(n, 1)\r\n",
        "\r\n",
        "  X = hstack((X1N, X2N, X3N))\r\n",
        "  np.random.shuffle(X)\r\n",
        "  X=X[1:128,:]\r\n",
        "  y = ones((128, 1))\r\n",
        "  y=y[1:128,:]\r\n",
        "  return X, y"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmyw6pNijri_"
      },
      "source": [
        "# generate points in latent space as input for the generator                   OK\r\n",
        "def generate_latent_points(latent_dim, n):\r\n",
        "\t# generate points in the latent space\r\n",
        "\tx_input = randn(latent_dim * n)\r\n",
        "\t# reshape into a batch of inputs for the network\r\n",
        "\tx_input = x_input.reshape(n, latent_dim)\r\n",
        "\treturn x_input"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F97ElmTjvTr"
      },
      "source": [
        "# use the generator to generate n fake examples, with class labels             OK\r\n",
        "def generate_fake_samples(generator, latent_dim, n):\r\n",
        "\t# generate points in latent space\r\n",
        "\tx_input = generate_latent_points(latent_dim, n)\r\n",
        "\t# predict outputs\r\n",
        "\tX = generator.predict(x_input)\r\n",
        "\t# create class labels\r\n",
        "\ty = zeros((n, 1))\r\n",
        "\treturn X, y"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaFRTVaWjxDl"
      },
      "source": [
        "# evaluate the discriminator and plot real and fake points\r\n",
        "def summarize_performance(epoch, generator, discriminator, latent_dim, n=123984):\r\n",
        "\t# prepare real samples\r\n",
        "  x_real, y_real = generate_real_samples(n)\r\n",
        "    # evaluate discriminator on real examples\r\n",
        "  _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\r\n",
        "    # prepare fake examples\r\n",
        "  x_fake, y_fake = generate_fake_samples(generator, latent_dim, 128)\r\n",
        "    # evaluate discriminator on fake examples\r\n",
        "  _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\r\n",
        "    # summarize discriminator performance\r\n",
        "  print(epoch, acc_real, acc_fake)\r\n",
        "\t# scatter plot real and fake data points\r\n",
        "\t#pyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\r\n",
        "\t#pyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\r\n",
        "\t#pyplot.show()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imF7tLEMjyud"
      },
      "source": [
        "# train the generator and discriminator\r\n",
        "def train(g_model, d_model, gan_model, latent_dim, n_epochs=1000, n_batch=128, n_eval=100):\r\n",
        "\t# determine half the size of one batch, for updating the discriminator\r\n",
        "  half_batch = 123984\r\n",
        "  #half_batch = int(n_batch / 2)\r\n",
        "  #half_batch = 100\r\n",
        "\t# manually enumerate epochs\r\n",
        "  for i in range(n_epochs):\r\n",
        "\t\t# prepare real samples\r\n",
        "    x_real, y_real = generate_real_samples(half_batch)\r\n",
        "        # prepare fake examples\r\n",
        "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_batch)\r\n",
        "        # update discriminator\r\n",
        "    d_model.train_on_batch(x_real, y_real)\r\n",
        "    d_model.train_on_batch(x_fake, y_fake)\r\n",
        "        # prepare points in latent space as input for the generator\r\n",
        "    x_gan = generate_latent_points(latent_dim, n_batch)\r\n",
        "        # create inverted labels for the fake samples\r\n",
        "    y_gan = ones((n_batch, 1))\r\n",
        "        # update the generator via the discriminator's error\r\n",
        "    gan_model.train_on_batch(x_gan, y_gan)\r\n",
        "        # evaluate the model every n_eval epochs\r\n",
        "    if (i+1) % n_eval == 0:\r\n",
        "      summarize_performance(i, g_model, d_model, latent_dim)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV0FdmHyjCRA",
        "outputId": "7ae0ce31-ab63-4c36-d585-51f1fdd62af9"
      },
      "source": [
        "# size of the latent space\r\n",
        "latent_dim = 4\r\n",
        "# create the discriminator\r\n",
        "discriminator = define_discriminator()\r\n",
        "# create the generator\r\n",
        "generator = define_generator(latent_dim)\r\n",
        "# create the gan\r\n",
        "gan_model = define_gan(generator, discriminator)\r\n",
        "# train model\r\n",
        "train(generator, discriminator, gan_model, latent_dim)\r\n",
        "\r\n",
        "gan_model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 0.8188976645469666 0.9140625\n",
            "199 0.6299212574958801 0.4921875\n",
            "299 0.913385808467865 0.6640625\n",
            "399 0.913385808467865 0.609375\n",
            "499 0.8582677245140076 0.7421875\n",
            "599 0.874015748500824 0.640625\n",
            "699 0.6062992215156555 0.953125\n",
            "799 0.6299212574958801 0.921875\n",
            "899 0.8425197005271912 0.9140625\n",
            "999 0.8661417365074158 0.6484375\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "sequential_19 (Sequential)   (None, 3)                 166787    \n",
            "_________________________________________________________________\n",
            "sequential_18 (Sequential)   (None, 1)                 166401    \n",
            "=================================================================\n",
            "Total params: 333,188\n",
            "Trainable params: 166,787\n",
            "Non-trainable params: 166,401\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgkiYNmSnXKX"
      },
      "source": [
        "latent_dim = 4\r\n",
        "n=100\r\n",
        "latent_points = generate_latent_points(latent_dim, n)\r\n",
        "# generate images\r\n",
        "X = generator.predict(latent_points)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj4QbCDT17b1",
        "outputId": "e2919242-16f6-4377-c177-40825062c4b9"
      },
      "source": [
        "maxX1=10000\r\n",
        "Var1 = (X[:,0]) * (maxX1 - minX1) + minX1\r\n",
        "Var1=np.abs(Var1)\r\n",
        "Var1=np.around(Var1)\r\n",
        "print(Var1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[253. 274. 108.  97. 314. 250. 122. 333. 233. 259.  30. 290.  74.  81.\n",
            " 283.  37. 264. 234. 244. 136. 292. 118.  96. 368. 275. 240.  87. 244.\n",
            " 116. 266. 108. 521. 267. 213. 117. 101. 294. 118.  96. 292.  60. 191.\n",
            " 113. 254. 296. 325. 304.  46. 233. 112. 163.  88. 124. 106. 119. 291.\n",
            " 118. 279. 378. 285. 342. 109. 279. 255. 123. 299.  77. 183. 118. 123.\n",
            " 146. 125. 114. 120. 234. 204. 400. 246. 161. 288. 102. 111. 137. 226.\n",
            " 119. 109. 106.  72. 130.  88. 242. 272.  88. 243. 404. 237. 227. 262.\n",
            " 113. 240.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0MQCadp3BQ0",
        "outputId": "0fc5cdd8-6ed3-45b0-fcba-f319db70118a"
      },
      "source": [
        "Var2 = X[:,1] * (maxX2 - minX2) + minX2\r\n",
        "Var2=np.abs(Var2)\r\n",
        "Var2=Var2*10000\r\n",
        "print(Var2)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1170288.8   3678811.     198656.52   198025.38  2987896.     683373.1\n",
            " 2733832.     898482.44   571126.4   1061760.4    542124.4    878490.5\n",
            " 1455369.6    322552.     823480.    2639489.2   5797199.    5379874.5\n",
            "  875824.44   401705.16  4774822.5    199896.61   301510.34  5434754.5\n",
            "  663344.1   5428922.5     30286.598 2654348.5    380349.97  5673703.\n",
            "  293516.12  1889956.6    594017.44  5879305.5   4134556.2    552098.3\n",
            " 1551315.     304426.53    77421.86   675271.3    396053.53  5870495.5\n",
            "   90883.41   584627.7    885462.6   1137760.2    790560.75   153117.08\n",
            "  552680.75   271045.78   608363.25   133070.45   191804.48   185438.78\n",
            "  309723.78   697589.44  2781236.5    698201.44  1117866.2    809690.9\n",
            " 3096103.5    170457.11   719962.     769749.3   4312292.    4559690.5\n",
            "  175639.11  5860567.5    303726.4   3090991.8   1739321.8   5446550.5\n",
            "  308833.78   223279.47  5017035.5   5877410.5   5880952.    2152967.\n",
            "  327311.72  5780125.5     77141.69   274180.75  1161882.    2500763.2\n",
            "  252149.42  1488643.5    115025.07   284840.75   167819.14   329802.72\n",
            "  571347.25   881265.7     81576.42   681183.3   5204296.5    934515.75\n",
            " 5881436.     685373.1    186988.45   676832.8  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8q46EcD5MTj",
        "outputId": "4c0eef67-69b3-4b87-8dac-f8cf98bb8417"
      },
      "source": [
        "Var3 = X[:,2]\r\n",
        "print(Var3)\r\n",
        "Var3[Var3 < 0] = -1\r\n",
        "Var3[Var3 > 0] = 1\r\n",
        "print(Var3)\r\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.9614283   0.96298724 -0.9863114  -0.9872285   0.9783973   0.9760811\n",
            " -0.98866165  0.98760337  0.9637038   0.9625438  -0.99887973  0.98115844\n",
            " -0.9939895  -0.99720794  0.97706884 -0.99866825  0.91124815  0.8991934\n",
            "  0.96689004 -0.992715    0.965044   -0.99106425 -0.9862808   0.98220295\n",
            "  0.98317474  0.90420985 -0.9904227   0.9534238  -0.9908644   0.93181735\n",
            " -0.99005175  0.99770725  0.9781572  -0.996018   -0.9895121  -0.9853768\n",
            "  0.9804852  -0.98668367 -0.988456    0.9812185  -0.99406576 -0.9990541\n",
            " -0.9894896   0.9723089   0.97984546  0.98651177  0.9821615  -0.9952781\n",
            "  0.962524   -0.9902821   0.4437755  -0.9924633  -0.98936915 -0.98657423\n",
            " -0.99170566  0.9798141  -0.99456006  0.9811562   0.991829    0.9779788\n",
            "  0.9835064  -0.99479586  0.9825265   0.97366536 -0.9904666   0.9686027\n",
            " -0.9936279  -0.995362   -0.9929345  -0.9871681  -0.46050686 -0.99327534\n",
            " -0.99262655 -0.993224    0.9211715  -0.9989103   0.9599641   0.96008176\n",
            " -0.2996071   0.9296023  -0.9879574  -0.9887335  -0.86214244  0.9474847\n",
            " -0.9929726  -0.98864776 -0.9876126  -0.995795   -0.9919858  -0.9888772\n",
            "  0.96805227  0.9780445  -0.9894139   0.9709366   0.98698765  0.96001923\n",
            " -0.9996293   0.9731782  -0.9901946   0.96592945]\n",
            "[ 1.  1. -1. -1.  1.  1. -1.  1.  1.  1. -1.  1. -1. -1.  1. -1.  1.  1.\n",
            "  1. -1.  1. -1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1. -1. -1. -1.\n",
            "  1. -1. -1.  1. -1. -1. -1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1.\n",
            " -1.  1. -1.  1.  1.  1.  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
            " -1. -1.  1. -1.  1.  1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
            "  1.  1. -1.  1.  1.  1. -1.  1. -1.  1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwdDUZaJSNEE"
      },
      "source": [
        "generator.save('/content/gdrive/My Drive/MMM/generator.h5')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgEkLspxSNGc"
      },
      "source": [
        "discriminator.save('/content/gdrive/My Drive/MMM/discriminator.h5')"
      ],
      "execution_count": 54,
      "outputs": []
    }
  ]
}